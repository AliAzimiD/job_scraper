
================================================================================
File Path: /home/rick/job-scraper/job_scraper/.dockerignore
================================================================================
# .dockerignore
job_data/
.git/
.gitignore
.env
*.pyc
__pycache__/
.pytest_cache/
.coverage
htmlcov/
.DS_Store


================================================================================
File Path: /home/rick/job-scraper/job_scraper/docker_manage.sh
================================================================================
#!/bin/bash

# docker_manage.sh
case "$1" in
    "build")
        docker-compose build
        ;;
    "start")
        docker-compose up -d
        ;;
    "stop")
        docker-compose down
        ;;
    "logs")
        docker-compose logs -f
        ;;
    "restart")
        docker-compose restart
        ;;
    "clean")
        docker-compose down -v
        ;;
    *)
        echo "Usage: $0 {build|start|stop|logs|restart|clean}"
        exit 1
        ;;
esac




================================================================================
File Path: /home/rick/job-scraper/job_scraper/Dockerfile
================================================================================
# Dockerfile
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    SCRAPER_ENV=production

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first to leverage Docker cache
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application
COPY . .

# Create necessary directories
RUN mkdir -p /app/job_data/raw_data \
    /app/job_data/processed_data \
    /app/job_data/logs

# Set permissions
RUN chmod -R 755 /app

# Run the application
CMD ["python", "main.py"]


================================================================================
File Path: /home/rick/job-scraper/job_scraper/docker-compose.dev.yml
================================================================================
version: '3.8'

services:
  scraper:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: job_scraper_dev
    volumes:
      - .:/app
      - ./job_data:/app/job_data
    environment:
      - SCRAPER_ENV=development
      - PYTHONPATH=/app
      - TZ=UTC
    command: python main.py
    ports:
      - "8888:8888"  # For debugging if needed
    restart: "no"


================================================================================
File Path: /home/rick/job-scraper/job_scraper/docker-compose.yml
================================================================================
# docker-compose.yml
version: '3.8'

services:
  scraper:
    build: .
    container_name: job_scraper
    volumes:
      - ./job_data:/app/job_data
      - ./config:/app/config
    environment:
      - SCRAPER_ENV=production
      - TZ=UTC
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"


================================================================================
File Path: /home/rick/job-scraper/job_scraper/Makefile
================================================================================
# Makefile

.PHONY: build start stop logs restart clean dev

build:
	docker-compose build

start:
	docker-compose up -d

stop:
	docker-compose down

logs:
	docker-compose logs -f

restart:
	docker-compose restart

clean:
	docker-compose down -v
	sudo rm -rf job_data/*

dev:
	docker-compose -f docker-compose.dev.yml up --build


================================================================================
File Path: /home/rick/job-scraper/job_scraper/CLEAN.ipynb
================================================================================



================================================================================
File Path: /home/rick/job-scraper/job_scraper/main.py
================================================================================
import asyncio
import logging
from pathlib import Path
from datetime import datetime
import signal
import sys
from src.scraper import JobScraper

class GracefulExit:
    def __init__(self):
        self.shutdown = False
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        print("\nShutdown signal received. Cleaning up...")
        self.shutdown = True

async def setup_logging():
    log_dir = Path("job_data/logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    log_file = log_dir / f"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logging.getLogger().handlers = []  # Add this line here
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)

async def main():
    # Initialize graceful exit handler
    graceful_exit = GracefulExit()
    
    # Setup logging
    logger = await setup_logging()
    logger.info("Starting job scraper...")

    try:
        # Initialize scraper
        scraper = JobScraper(
            config_path="config/api_config.yaml",
            save_dir="job_data"
        )
        
        # Run scraper until completion or interruption
        while not graceful_exit.shutdown:
            try:
                await scraper.scrape()
                logger.info("Scraping completed successfully")
                break
            except Exception as e:
                logger.error(f"Error during scraping: {str(e)}")
                if not graceful_exit.shutdown:
                    logger.info("Retrying in 60 seconds...")
                    await asyncio.sleep(60)
                else:
                    break
        
    except Exception as e:
        logger.error(f"Critical error: {str(e)}")
        sys.exit(1)
    finally:
        if graceful_exit.shutdown:
            logger.info("Graceful shutdown completed")
        
        # Cleanup code here if needed
        await asyncio.sleep(1)  # Allow final logs to be written

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nScript terminated by user")
    except Exception as e:
        print(f"Fatal error: {str(e)}")
        sys.exit(1)


================================================================================
File Path: /home/rick/job-scraper/job_scraper/exractor.ipynb
================================================================================


================================================================================
File Path: /home/rick/job-scraper/job_scraper/exractor.txt
================================================================================

================================================================================
File Path: /home/rick/job-scraper/job_scraper/.dockerignore
================================================================================
# .dockerignore
job_data/
.git/
.gitignore
.env
*.pyc
__pycache__/
.pytest_cache/
.coverage
htmlcov/
.DS_Store


================================================================================
File Path: /home/rick/job-scraper/job_scraper/docker_manage.sh
================================================================================
#!/bin/bash

# docker_manage.sh
case "$1" in
    "build")
        docker-compose build
        ;;
    "start")
        docker-compose up -d
        ;;
    "stop")
        docker-compose down
        ;;
    "logs")
        docker-compose logs -f
        ;;
    "restart")
        docker-compose restart
        ;;
    "clean")
        docker-compose down -v
        ;;
    *)
        echo "Usage: $0 {build|start|stop|logs|restart|clean}"
        exit 1
        ;;
esac


================================================================================
File Path: /home/rick/job-scraper/job_scraper/jobcategories_map.json
================================================================================


================================================================================
File Path: /home/rick/job-scraper/job_scraper/Dockerfile
================================================================================
# Dockerfile
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    SCRAPER_ENV=production

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first to leverage Docker cache
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application
COPY . .

# Create necessary directories
RUN mkdir -p /app/job_data/raw_data \
    /app/job_data/processed_data \
    /app/job_data/logs

# Set permissions
RUN chmod -R 755 /app

# Run the application
CMD ["python", "main.py"]


================================================================================
File Path: /home/rick/job-scraper/job_scraper/docker-compose.dev.yml
================================================================================
version: '3.8'

services:
  scraper:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: job_scraper_dev
    volumes:
      - .:/app
      - ./job_data:/app/job_data
    environment:
      - SCRAPER_ENV=development
      - PYTHONPATH=/app
      - TZ=UTC
    command: python main.py
    ports:
      - "8888:8888"  # For debugging if needed
    restart: "no"


================================================================================
File Path: /home/rick/job-scraper/job_scraper/docker-compose.yml
================================================================================
# docker-compose.yml
version: '3.8'

services:
  scraper:
    build: .
    container_name: job_scraper
    volumes:
      - ./job_data:/app/job_data
      - ./config:/app/config
    environment:
      - SCRAPER_ENV=production
      - TZ=UTC
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"


================================================================================
File Path: /home/rick/job-scraper/job_scraper/Makefile
================================================================================
# Makefile

.PHONY: build start stop logs restart clean dev

build:
	docker-compose build

start:
	docker-compose up -d

stop:
	docker-compose down

logs:
	docker-compose logs -f

restart:
	docker-compose restart

clean:
	docker-compose down -v
	sudo rm -rf job_data/*

dev:
	docker-compose -f docker-compose.dev.yml up --build


================================================================================
File Path: /home/rick/job-scraper/job_scraper/CLEAN.ipynb
================================================================================
import os
import re
import ast
import json
import pandas as pd
from sqlalchemy import create_engine

# 1) Define your source folder containing all CSV batch files.
path_to_directory = "/home/rick/job-scraper/job_scraper/job_data/processed_data/"

# 2) List all CSV files that match your "batch_xxx.csv" naming pattern.
all_csvs = [f for f in os.listdir(path_to_directory) if f.startswith("batch_") and f.endswith(".csv")]
all_csvs.sort()

# 3) Define the columns you want to keep.
keep_cols = [
    'id', 'title', 'url', 'locations', 'workTypes', 'salary', 'gender', 'tags',
    'itemIndex', 'jobPostCategories', 'companyFaName', 'provinceMatchCity',
    'normalizeSalaryMin', 'normalizeSalaryMax', 'paymentMethod', 'district',
    'company.titleFa', 'jobBoard.id', 'jobBoard.titleEn', 'activationTime.date',
    'companyDetailsSummary.id', 'companyDetailsSummary.name.titleFa',
    'companyDetailsSummary.name.titleEn', 'companyDetailsSummary.about.titleFa',
    'companyDetailsSummary.url'
]

# 4) Prepare a mapping for text-based tags -> numeric codes.
tag_map = {
    'پاره وقت': 1,
    'بدون نیاز به سابقه': 2,
    'پروژه ای': 3,
    'کارآموزی': 4
}

def convert_tags_to_numbers(tags_string):
    """Convert text tags to numeric codes."""
    if not isinstance(tags_string, str) or tags_string.strip() == "[]":
        return "0"
    try:
        tags_list = ast.literal_eval(tags_string)
    except:
        return "0"
    if not isinstance(tags_list, list) or len(tags_list) == 0:
        return "0"
    numeric_tags = [tag_map.get(t, 0) for t in tags_list]
    unique_sorted_codes = sorted(set(numeric_tags))
    return ",".join(str(x) for x in unique_sorted_codes)

def extract_location_ids(row):
    """Extract province and city IDs from 'locations'."""
    try:
        locs = ast.literal_eval(row)
        if isinstance(locs, list):
            location_ids = []
            for loc in locs:
                if 'province' in loc and 'id' in loc['province']:
                    location_ids.append(loc['province']['id'])
                if 'city' in loc and 'id' in loc['city']:
                    location_ids.append(loc['city']['id'])
            return location_ids
        return []
    except:
        return []

def fix_json_format(row):
    """Replace single quotes with double quotes and None with null."""
    if pd.isna(row):
        return None
    row = re.sub(r"'", r'"', row)  # Replace single quotes with double quotes
    row = row.replace("None", "null")  # Replace None with null
    return row

###############################################################################
#  Here is our global category_map. We will fill it whenever we parse categories.
###############################################################################
category_map = {}

def parse_and_store_categories(obj):
    """
    Recursively traverse the category JSON (dict/list),
    extract relevant fields, and store in category_map.
    """
    if isinstance(obj, dict):
        # If the object looks like a category item
        if 'id' in obj and isinstance(obj['id'], int):
            cat_id = obj['id']
            # Add or update an entry in category_map
            category_map[cat_id] = {
                "parentId": obj.get("parentId"),
                "titleFa": obj.get("titleFa"),
                "titleEn": obj.get("titleEn"),
                "urlParameter": obj.get("urlParameter"),
                "order": obj.get("order"),
                "seoOrder": obj.get("seoOrder"),
                # If you want to store other fields, add them here
            }
        # Recurse into children
        if 'children' in obj and isinstance(obj['children'], list):
            for child in obj['children']:
                parse_and_store_categories(child)
    elif isinstance(obj, list):
        for item in obj:
            parse_and_store_categories(item)

def extract_category_ids(row):
    """Extract all category IDs as a comma-separated string, also fill category_map."""
    try:
        data = json.loads(row)
        ids = []

        def get_ids(obj):
            # We'll keep the original ID extraction logic
            if isinstance(obj, dict):
                if 'id' in obj and isinstance(obj['id'], int):
                    ids.append(obj['id'])
                if 'children' in obj and isinstance(obj['children'], list):
                    for child in obj['children']:
                        get_ids(child)
            elif isinstance(obj, list):
                for item in obj:
                    get_ids(item)

        # 1) Extract IDs in a local list
        get_ids(data)

        # 2) Also parse & store the categories in category_map
        parse_and_store_categories(data)
        zero_padded_ids = [f"{i:03d}" for i in sorted(set(ids))]
        
        return ",".join(zero_padded_ids)
    except Exception:
        return ""

# 5) Read all CSVs into a single DataFrame
df_list = []
for csv_file in all_csvs:
    csv_path = os.path.join(path_to_directory, csv_file)
    print("Reading file:", csv_path)
    temp_df = pd.read_csv(csv_path, low_memory=False)
    temp_df = temp_df.reindex(columns=keep_cols)
    df_list.append(temp_df)

df_all = pd.concat(df_list, ignore_index=True)

# 6) Create the new columns.

# 6.1) location_ids + location_ids_string
df_all['location_ids'] = df_all['locations'].apply(extract_location_ids)
df_all['location_ids_string'] = df_all['location_ids'].apply(
    lambda x: ",".join(f"{id_:03d}" for id_ in x)
)

# 6.2) jobPostCategories_string (ID extraction)
df_all['jobPostCategories_fixed'] = df_all['jobPostCategories'].apply(fix_json_format)
df_all['jobPostCategories_string'] = df_all['jobPostCategories_fixed'].apply(extract_category_ids)

# 6.3) tags -> tag_number
df_all['tag_number'] = df_all['tags'].apply(convert_tags_to_numbers)

# 7) Convert list columns to JSON strings if desired
df_all['location_ids'] = df_all['location_ids'].apply(json.dumps)

# 8) Drop original JSON columns you no longer need
df_all.drop(
    ['locations', 'jobPostCategories', 'tags', 'jobPostCategories_fixed'],
    axis=1, inplace=True, errors='ignore'
)

# Define the path where you want to save the database
db_path = "/home/rick/job-scraper/job_scraper/metabase/my_cleaned_data_for_metabase.db"

# Create the engine with the full path
engine = create_engine(f"sqlite:///{db_path}", echo=False)

# Save the DataFrame into the SQLite database
df_all.to_sql("cleaned_jobs", con=engine, if_exists="replace", index=False)

print("All CSV files have been processed and written to 'cleaned_jobs' table in 'my_cleaned_data.db'.")

###############################################################################
# 10) Now export the 'category_map' to a separate JSON file (or CSV).
###############################################################################
output_map_path = "jobcategories_map.json"

with open(output_map_path, "w", encoding="utf-8") as f:
    # Dump the dictionary with pretty indentation and no ASCII-escaping
    json.dump(category_map, f, ensure_ascii=False, indent=2)

print(f"Category map has been exported to '{output_map_path}' with {len(category_map)} entries.")



================================================================================
File Path: /home/rick/job-scraper/job_scraper/requirements.txt
================================================================================
aiohttp==3.9.3
pandas==2.2.0
pyyaml==6.0.1
tenacity==8.2.3
pyarrow==15.0.0  # for parquet support


================================================================================
File Path: /home/rick/job-scraper/job_scraper/src/scheduler.py
================================================================================
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional
from .scraper import JobScraper

class JobScraperScheduler:
    def __init__(self,
                 config_path: str = "config/api_config.yaml",
                 base_dir: str = "job_data",
                 interval_minutes: int = 30):
        """Initialize the scheduler"""
        self.config_path = config_path
        self.base_dir = Path(base_dir)
        self.interval_minutes = interval_minutes
        self.logger = logging.getLogger('JobScraperScheduler')

    async def run(self) -> None:
        """Run the scraper at regular intervals"""
        while True:
            try:
                self.logger.info("Starting scraping run")
                start_time = datetime.now()
                
                # Initialize and run scraper
                scraper = JobScraper(
                    config_path=self.config_path,
                    save_dir=self.base_dir
                )
                await scraper.scrape()
                
                # Log completion
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                self.logger.info(
                    f"Completed scraping run. Duration: {duration:.2f} seconds. "
                    f"Jobs collected: {scraper.total_jobs_scraped}"
                )
                
                # Wait for next interval
                self.logger.info(f"Waiting {self.interval_minutes} minutes until next run")
                await asyncio.sleep(self.interval_minutes * 60)
                
            except Exception as e:
                self.logger.error(f"Error in scheduler: {str(e)}")
                await asyncio.sleep(60)  # Wait a minute before retrying


================================================================================
File Path: /home/rick/job-scraper/job_scraper/src/__init__.py
================================================================================


================================================================================
File Path: /home/rick/job-scraper/job_scraper/src/scraper.py
================================================================================
import aiohttp
import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union, Set
import pandas as pd
from tenacity import retry, stop_after_attempt, wait_exponential, RetryError
import traceback
from .config_manager import ConfigManager

class JobScraper:
    def __init__(self, 
                 config_path: str = "config/api_config.yaml",
                 save_dir: str = "job_data"):
        """Initialize JobScraper with configuration"""
        # Load configuration
        self.config_manager = ConfigManager(config_path)
        self.api_config = self.config_manager.api_config
        self.request_config = self.config_manager.request_config
        self.scraper_config = self.config_manager.scraper_config
        
        # Initialize attributes
        self.base_url = self.api_config['base_url']
        self.headers = self.api_config['headers']
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup directories
        self.raw_dir = self.save_dir / "raw_data"
        self.processed_dir = self.save_dir / "processed_data"
        self.log_dir = self.save_dir / "logs"
        
        for dir_path in [self.raw_dir, self.processed_dir, self.log_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # Setup logging
        self.logger = self._setup_logging()
        
        # Initialize state
        self.processed_job_ids: Set[str] = set()
        self.last_item_index = 0
        self.earliest_date: Optional[datetime] = None
        self.latest_date: Optional[datetime] = None
        self.current_batch = 0
        self.total_jobs_scraped = 0
        self.failed_requests: List[int] = []
        self.semaphore = asyncio.Semaphore(
            self.scraper_config.get('max_concurrent_requests', 3)
        )
        
        # Load existing state
        self._load_existing_state()
        
    async def _handle_error(self, page: int, error: Exception) -> None:
        """Handle scraping errors with logging and tracking"""
        error_msg = str(error)
        self.logger.error(f"Error on page {page}: {error_msg}")
        self.logger.error(traceback.format_exc())
        
        self.failed_requests.append(page)
        
        # Update state with error information
        error_state = {
            'last_error': {
                'page': page,
                'error': error_msg,
                'timestamp': datetime.now().isoformat()
            },
            'failed_requests': self.failed_requests
        }
        
        try:
            self.config_manager.save_state(error_state)
        except Exception as e:
            self.logger.error(f"Failed to save error state: {str(e)}")
        
        # Check if we should retry based on error type
        if isinstance(error, (aiohttp.ClientError, asyncio.TimeoutError)):
            retry_delay = self.scraper_config.get('error_sleep_time', 2)
            self.logger.info(f"Will retry page {page} after {retry_delay} seconds")
            await asyncio.sleep(retry_delay)
        else:
            self.logger.error(f"Unrecoverable error on page {page}")
            raise
    async def _log_final_statistics(self, pages_processed: int) -> None:
        """Log final scraping statistics"""
        stats = {
            'total_jobs_scraped': self.total_jobs_scraped,
            'pages_processed': pages_processed,
            'failed_requests': len(self.failed_requests),
            'unique_jobs': len(self.processed_job_ids),
            'end_time': datetime.now().isoformat()
        }
        
        self.logger.info("Scraping completed. Final statistics:")
        for key, value in stats.items():
            self.logger.info(f"{key}: {value}")
        
        try:
            self.config_manager.save_state({
                'last_run_stats': stats,
                'scraping_complete': True
            })
        except Exception as e:
            self.logger.error(f"Failed to save final statistics: {str(e)}")

    def _setup_logging(self) -> logging.Logger:
        """Configure logging with both file and console handlers"""
        logger = logging.getLogger('JobScraper')
        logger.propagate = False  # Add this line
        
        if not logger.handlers:
            logger.setLevel(logging.INFO)
            
            # Create formatters and handlers
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            
            # File handler
            log_file = self.log_dir / f"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
            fh = logging.FileHandler(log_file)
            fh.setLevel(logging.INFO)
            fh.setFormatter(formatter)
            
            # Console handler
            ch = logging.StreamHandler()
            ch.setLevel(logging.INFO)
            ch.setFormatter(formatter)
            
            logger.addHandler(fh)
            logger.addHandler(ch)
    
        return logger

    def _load_existing_state(self) -> None:
        """Load existing state from saved files"""
        try:
            json_files = sorted(self.raw_dir.glob("batch_*.json"))
            
            if not json_files:
                self.logger.info("No existing files found. Starting fresh.")
                return
            
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        jobs = json.load(f)
                        
                    for job in jobs:
                        self.processed_job_ids.add(job['id'])
                        
                        if job.get('itemIndex', 0) > self.last_item_index:
                            self.last_item_index = job['itemIndex']
                        
                        # Handle different date formats
                        date_str = job['activationTime']['date']
                        try:
                            # Try the format with microseconds
                            job_date = datetime.strptime(date_str, "%Y-%m-%dT%H:%M:%S.%f")
                        except ValueError:
                            try:
                                # Try the format without microseconds
                                job_date = datetime.strptime(date_str, "%Y-%m-%dT%H:%M:%S")
                            except ValueError:
                                # Try just the date and time
                                job_date = datetime.strptime(date_str, "%Y-%m-%dT%H:%M")
                        
                        if not self.earliest_date or job_date < self.earliest_date:
                            self.earliest_date = job_date
                        if not self.latest_date or job_date > self.latest_date:
                            self.latest_date = job_date
                                
                except Exception as e:
                    self.logger.error(f"Error loading file {json_file}: {str(e)}")
                    continue
                        
            self.logger.info(f"Loaded {len(self.processed_job_ids)} existing job IDs")
            if self.earliest_date and self.latest_date:
                self.logger.info(f"Date range: {self.earliest_date} to {self.latest_date}")
            self.logger.info(f"Last item index: {self.last_item_index}")
                
        except Exception as e:
            self.logger.error(f"Error loading existing state: {str(e)}")
            raise

    def create_payload(self, page: int = 1) -> Dict:
        """Create API request payload"""
        payload = self.request_config['default_payload'].copy()
        payload.update({
            'page': page,
            'pageSize': self.scraper_config.get('batch_size', 100),
            'nextPageToken': None
            # Removed 'last_index' to avoid limiting results
        })
        return payload

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def fetch_jobs(
        self,
        session: aiohttp.ClientSession,
        params: dict,
        page: int
    ) -> Optional[Dict]:
        """Fetch jobs with retry logic"""
        async with self.semaphore:
            try:
                async with session.post(
                    self.base_url,
                    headers=self.headers,
                    json=params,
                    timeout=self.scraper_config.get('timeout', 60)
                ) as response:
                    response.raise_for_status()
                    data = await response.json()
                    self.logger.info(f"Successfully fetched page {page}")
                    self.logger.debug(f"Retrieved {len(data.get('data', {}).get('jobPosts', []))} jobs from page {page}")
                    return data
            except aiohttp.ClientError as e:
                self.logger.error(f"Network error fetching page {page}: {str(e)}")
                raise
            except json.JSONDecodeError as e:
                self.logger.error(f"JSON decode error on page {page}: {str(e)}")
                raise
            except Exception as e:
                self.logger.error(f"Unexpected error fetching page {page}: {str(e)}")
                raise

    async def process_jobs(self, jobs: List[dict]) -> List[dict]:
        """Process and validate job data"""
        processed = []
        
        for job in jobs:
            try:
                # Skip if we've seen this job before
                if job['id'] in self.processed_job_ids:
                    continue
                    
                # Basic validation
                required_fields = ['id', 'title', 'activationTime']
                if not all(field in job for field in required_fields):
                    self.logger.warning(
                        f"Skipping invalid job: {job.get('id', 'unknown')} - "
                        f"Missing required fields"
                    )
                    continue
                    
                # Add to processed set and list
                self.processed_job_ids.add(job['id'])
                processed.append(job)
                
            except Exception as e:
                self.logger.error(f"Error processing job: {str(e)}")
                continue
        
        return processed

    def save_batch(self, jobs: List[Dict], batch_number: int) -> None:
        """Save a batch of jobs to file"""
        if not jobs:
            return
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        batch_name = f"batch_{batch_number:04d}_{timestamp}"
        
        try:
            # Save raw JSON
            json_path = self.raw_dir / f"{batch_name}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(jobs, f, ensure_ascii=False, indent=2)
            
            # Convert to DataFrame and save processed data
            df = pd.json_normalize(jobs)
            
            # Save as Parquet
            parquet_path = self.processed_dir / f"{batch_name}.parquet"
            df.to_parquet(parquet_path, index=False)
            
            # Save as CSV
            csv_path = self.processed_dir / f"{batch_name}.csv"
            df.to_csv(csv_path, index=False, encoding='utf-8')
            
            self.logger.info(
                f"Saved batch {batch_number} with {len(jobs)} jobs - "
                f"Total jobs: {self.total_jobs_scraped}"
            )
            
        except Exception as e:
            self.logger.error(f"Error saving batch {batch_number}: {str(e)}")
            self.logger.error(traceback.format_exc())

    async def scrape(self) -> None:
        """Main scraping method that handles both resume functionality and new job detection"""
        async with aiohttp.ClientSession() as session:
            # Load previous state and job tracking info
            last_state = self.config_manager.load_state()
            start_page = last_state.get('last_page_scraped', 1)
            last_run_timestamp = None
            
            if last_state.get('last_run'):
                try:
                    last_run_timestamp = datetime.fromisoformat(last_state['last_run'])
                    time_difference = datetime.now() - last_run_timestamp
                    
                    # If it's been more than configured hours, start fresh
                    if time_difference.total_seconds() > self.scraper_config.get('max_resume_age', 86400):
                        self.logger.info(f"Last run was {time_difference.total_seconds()/3600:.2f} hours ago. Starting fresh scan")
                        start_page = 1
                    else:
                        # Start a few pages back to catch new insertions
                        lookback_pages = self.scraper_config.get('lookback_pages', 5)
                        start_page = max(1, start_page - lookback_pages)
                        self.logger.info(f"Resuming from page {start_page} (with {lookback_pages} pages lookback)")
                except ValueError:
                    self.logger.warning("Invalid last run timestamp. Starting fresh scan")
                    start_page = 1
                    
            page = start_page
            batch_num = len(list(self.raw_dir.glob("batch_*.json"))) + 1
            current_batch_jobs = []
            max_pages = self.scraper_config.get('max_pages', 1000)
            consecutive_empty_pages = 0
            max_empty_pages = self.scraper_config.get('max_empty_pages', 3)
            new_jobs_found = False
            
            # Track the most recent job timestamp we've seen
            most_recent_timestamp = last_run_timestamp
            
            while page <= max_pages:
                try:
                    params = self.create_payload(page=page)
                    
                    result = await self.fetch_jobs(session, params, page)
                    
                    if not result or not result.get('data', {}).get('jobPosts'):
                        self.logger.info("No more jobs found in API response")
                        break
                    
                    jobs = result['data']['jobPosts']
                    self.logger.info(f"Retrieved {len(jobs)} jobs from page {page}")
                    
                    if not jobs:
                        consecutive_empty_pages += 1
                        self.logger.info(f"Empty page {page} (empty count: {consecutive_empty_pages})")
                        if consecutive_empty_pages >= max_empty_pages and page > start_page:
                            self.logger.info(f"Received {max_empty_pages} consecutive empty pages. Stopping.")
                            break
                        page += 1
                        continue
                    
                    # Process jobs and track timestamps
                    processed_jobs = await self.process_jobs(jobs)
                    
                    # Update most recent timestamp
                    for job in jobs:
                        job_timestamp = await self._parse_job_timestamp(job)  # Added await here
                        if job_timestamp and (not most_recent_timestamp or job_timestamp > most_recent_timestamp):
                            most_recent_timestamp = job_timestamp
                    
                    if processed_jobs:
                        new_jobs_found = True
                        consecutive_empty_pages = 0
                        current_batch_jobs.extend(processed_jobs)
                        
                        # Save batch if threshold reached
                        if len(current_batch_jobs) >= self.scraper_config.get('jobs_per_batch', 500):
                            await self._save_batch_with_state(
                                current_batch_jobs, 
                                batch_num,
                                page,
                                most_recent_timestamp
                            )
                            batch_num += 1
                            current_batch_jobs = []
                    else:
                        consecutive_empty_pages += 1
                        self.logger.info(f"No new jobs on page {page} (empty count: {consecutive_empty_pages})")
                        
                        # Check if we should continue based on job timestamps
                        oldest_job_timestamp = await self._get_oldest_job_timestamp(jobs)  # Added await here
                        if oldest_job_timestamp and last_run_timestamp:
                            if oldest_job_timestamp < last_run_timestamp and consecutive_empty_pages >= max_empty_pages:
                                self.logger.info("Reached older jobs with no new content. Stopping.")
                                break
                    
                    # Continue if we're finding new jobs or still in initial pages
                    if new_jobs_found or page - start_page < self.scraper_config.get('minimum_pages', 10):
                        page += 1
                        await asyncio.sleep(self.scraper_config.get('sleep_time', 1))
                    else:
                        if consecutive_empty_pages >= max_empty_pages:
                            self.logger.info("No new jobs found in recent pages. Stopping.")
                            break
                        page += 1
                    
                except Exception as e:
                    await self._handle_error(page, e)
                    
                    if len(self.failed_requests) >= self.scraper_config.get('max_retries', 5):
                        self.logger.error(f"Too many failed requests ({len(self.failed_requests)}). Stopping.")
                        break
                    
                    await asyncio.sleep(self.scraper_config.get('error_sleep_time', 2))
            
            # Save any remaining jobs in the final batch
            if current_batch_jobs:
                await self._save_batch_with_state(
                    current_batch_jobs, 
                    batch_num, 
                    page,
                    most_recent_timestamp
                )
            
            await self._log_final_statistics(page - start_page)

    async def _parse_job_timestamp(self, job: Dict) -> Optional[datetime]:
        """Extract and parse job timestamp"""
        try:
            # Handle the specific structure of your job timestamps
            if 'activationTime' in job and 'date' in job['activationTime']:
                timestamp_str = job['activationTime']['date']
                # Try different timestamp formats
                for fmt in [
                    "%Y-%m-%dT%H:%M:%S.%f",
                    "%Y-%m-%dT%H:%M:%S",
                    "%Y-%m-%dT%H:%M"
                ]:
                    try:
                        return datetime.strptime(timestamp_str, fmt)
                    except ValueError:
                        continue
                
                self.logger.warning(f"Could not parse timestamp {timestamp_str} for job {job.get('id')}")
                return None
            else:
                self.logger.warning(f"No timestamp found for job {job.get('id')}")
                return None
                
        except Exception as e:
            self.logger.warning(f"Error parsing timestamp for job {job.get('id')}: {str(e)}")
            return None

    async def _get_oldest_job_timestamp(self, jobs: List[Dict]) -> Optional[datetime]:
        """Get the oldest timestamp from a list of jobs"""
        timestamps = []
        for job in jobs:
            timestamp = await self._parse_job_timestamp(job)  # Ensure we await the timestamp parsing
            if timestamp:
                timestamps.append(timestamp)
        return min(timestamps) if timestamps else None


    async def _save_batch_with_state(
        self, 
        jobs: List[Dict], 
        batch_num: int, 
        current_page: int, 
        most_recent_timestamp: Optional[datetime]
    ) -> None:
        """Save batch and update state atomically"""
        self.save_batch(jobs, batch_num)
        self.total_jobs_scraped += len(jobs)
        self.logger.info(
            f"Saved batch {batch_num} with {len(jobs)} jobs - "
            f"Total jobs: {self.total_jobs_scraped}"
        )
        
        # Update state with batch and timestamp information
        current_state = {
            'last_page_scraped': current_page,
            'total_jobs_scraped': self.total_jobs_scraped,
            'last_batch_num': batch_num,
            'last_run': datetime.now().isoformat(),
            'most_recent_job_timestamp': (
                most_recent_timestamp.isoformat() 
                if most_recent_timestamp else None
            )
        }
        self.config_manager.save_state(current_state)
        self.logger.debug(f"Updated state after batch save: {current_state}")

    async def _log_final_statistics(self, pages_processed: int) -> None:
        """Log final scraping statistics"""
        stats = {
            'total_jobs_scraped': self.total_jobs_scraped,
            'pages_processed': pages_processed,
            'failed_requests': len(self.failed_requests),
            'unique_jobs': len(self.processed_job_ids),
            'end_time': datetime.now().isoformat()
        }
        
        self.logger.info("Scraping completed. Final statistics:")
        for key, value in stats.items():
            self.logger.info(f"{key}: {value}")
        
        try:
            self.config_manager.save_state({
                'last_run_stats': stats,
                'scraping_complete': True
            })
        except Exception as e:
            self.logger.error(f"Failed to save final statistics: {str(e)}")

    async def _handle_error(self, page: int, error: Exception) -> None:
        """Handle scraping errors with logging and tracking"""
        error_msg = str(error)
        self.logger.error(f"Error on page {page}: {error_msg}")
        self.logger.error(traceback.format_exc())
        
        self.failed_requests.append(page)
        
        # Update state with error information
        error_state = {
            'last_error': {
                'page': page,
                'error': error_msg,
                'timestamp': datetime.now().isoformat()
            },
            'failed_requests': self.failed_requests
        }
        
        try:
            self.config_manager.save_state(error_state)
        except Exception as e:
            self.logger.error(f"Failed to save error state: {str(e)}")
        
        # Check if we should retry based on error type
        if isinstance(error, (aiohttp.ClientError, asyncio.TimeoutError)):
            retry_delay = self.scraper_config.get('error_sleep_time', 2)
            self.logger.info(f"Will retry page {page} after {retry_delay} seconds")
            await asyncio.sleep(retry_delay)
        else:
            self.logger.error(f"Unrecoverable error on page {page}")
            raise

================================================================================
File Path: /home/rick/job-scraper/job_scraper/src/config_manager.py
================================================================================
import os
import yaml
from pathlib import Path
from typing import Dict, Any, Optional
import logging
from datetime import datetime
import json

class ConfigManager:
    def __init__(self, config_path: str = "config/api_config.yaml"):
        """Initialize ConfigManager with path to config file"""
        self.env = os.getenv('SCRAPER_ENV', 'development')
        self.config_path = Path(config_path)
        self.logger = self._setup_logging()
        self.config = self._load_config()
        self.state_file = Path("job_data/state/scraper_state.json")
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        self.state = self._load_state()

    def _setup_logging(self) -> logging.Logger:
        """Setup logging for config manager"""
        logger = logging.getLogger('ConfigManager')
        logger.propagate = False
        
        if not logger.handlers:
            logger.setLevel(logging.INFO)
            
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            
            ch = logging.StreamHandler()
            ch.setLevel(logging.INFO)
            ch.setFormatter(formatter)
            logger.addHandler(ch)
        
        return logger

    def _load_config(self) -> Dict[str, Any]:
        """Load configuration from YAML file"""
        try:
            if not self.config_path.exists():
                raise FileNotFoundError(f"Configuration file not found: {self.config_path}")
                
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            self._validate_config(config)
            return config
            
        except Exception as e:
            self.logger.error(f"Error loading configuration: {str(e)}")
            raise

    def load_state(self) -> Dict[str, Any]:
        """Public method to load state - this is what the scraper will call"""
        return self._load_state()

    def _load_state(self) -> Dict[str, Any]:
        """Internal method to load scraper state from JSON file"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r') as f:
                    state = json.load(f)
                self.logger.info(f"Loaded state: last page {state.get('last_page_scraped', 0)}, "
                               f"total jobs {state.get('total_jobs_scraped', 0)}")
                return state
            except Exception as e:
                self.logger.error(f"Error loading state: {str(e)}")
                return self._create_default_state()
        return self._create_default_state()

    def _create_default_state(self) -> Dict[str, Any]:
        """Create default state structure"""
        default_state = {
            'last_run': None,
            'total_jobs_scraped': 0,
            'last_page_scraped': 1,
            'last_job_id': None,
            'errors': [],
            'stats': {
                'success_count': 0,
                'error_count': 0,
                'retry_count': 0
            }
        }
        self.logger.info("Created default state")
        return default_state

    def _validate_config(self, config: Dict[str, Any]) -> None:
        """Validate configuration structure"""
        required_sections = ['api', 'request', 'scraper']
        required_api_fields = ['base_url', 'headers']
        required_scraper_fields = [
            'jobs_per_batch', 'sleep_time', 'max_retries',
            'timeout', 'batch_size', 'max_concurrent_requests'
        ]

        for section in required_sections:
            if section not in config:
                raise ValueError(f"Missing required section: {section}")

        for field in required_api_fields:
            if field not in config['api']:
                raise ValueError(f"Missing required API field: {field}")

        for field in required_scraper_fields:
            if field not in config['scraper']:
                raise ValueError(f"Missing required scraper field: {field}")

    def save_state(self, state_update: Dict[str, Any]) -> None:
        """Update and save scraper state"""
        try:
            self.state.update(state_update)
            self.state['last_updated'] = datetime.now().isoformat()
            
            # Create parent directories if they don't exist
            self.state_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(self.state_file, 'w') as f:
                json.dump(self.state, f, indent=2)
                
            self.logger.info(f"Saved state: page {self.state.get('last_page_scraped')}, "
                           f"total jobs {self.state.get('total_jobs_scraped')}")
                
        except Exception as e:
            self.logger.error(f"Error saving state: {str(e)}")

    @property
    def api_config(self) -> Dict[str, Any]:
        """Get API configuration"""
        return self.config.get('api', {})

    @property
    def request_config(self) -> Dict[str, Any]:
        """Get request configuration"""
        return self.config.get('request', {})

    @property
    def scraper_config(self) -> Dict[str, Any]:
        """Get scraper configuration"""
        return self.config.get('scraper', {})

    def update_config(self, new_config: Dict[str, Any]) -> None:
        """Update configuration and save to file"""
        try:
            merged_config = self.config.copy()
            merged_config.update(new_config)
            self._validate_config(merged_config)
            
            self.config = merged_config
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, allow_unicode=True)
                
            self.logger.info("Configuration updated successfully")
            
        except Exception as e:
            self.logger.error(f"Error updating configuration: {str(e)}")
            raise

    def get_state(self) -> Dict[str, Any]:
        """Get current scraper state"""
        return self.state.copy()

    def reset_state(self) -> None:
        """Reset scraper state to default values"""
        self.state = self._create_default_state()
        self.save_state({})
        self.logger.info("State reset to default values")


================================================================================
File Path: /home/rick/job-scraper/job_scraper/config/api_config.yaml
================================================================================
api:
  base_url: "https://api.karbord.io/api/v1/Candidate/JobPost/GetList"
  headers:
    accept: "application/json, text/plain, */*"
    accept-encoding: "gzip, deflate, br, zstd"
    accept-language: "en-US,en;q=0.9"
    clientid: "4558668"
    content-type: "application/json"
    ngsw-bypass: "true"
    origin: "https://karbord.io"
    referer: "https://karbord.io/"
    sec-ch-ua: '"Not(A:Brand";v="99", "Google Chrome";v="133", "Chromium";v="133"'
    sec-ch-ua-mobile: "?0"
    sec-ch-ua-platform: '"Windows"'
    sec-fetch-dest: "empty"
    sec-fetch-mode: "cors"
    sec-fetch-site: "same-site"
    user-agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"

request:
  default_payload:
    isInternship: false
    isRemote: false
    location: null
    publishDate: null
    workType: null
    pageSize: 100
    sort: 0
    searchId: null
    jobPostCategories: []
    jobBoardIds: []
    hasNoWorkExperienceRequirement: false
    clientId: 4558668
    page: 1
    nextPageToken: null

scraper:
  # Core scraping parameters
  batch_size: 100  # API limit per request
  jobs_per_batch: 1000  # Number of jobs to accumulate before saving
  max_pages: 9000  # Maximum pages to scrape
  chunk_size: 1000  # Size of job chunks for processing
  
  # Performance and resource settings
  memory_limit: 1024  # Maximum memory usage in MB
  max_concurrent_requests: 5  # Number of concurrent requests
  timeout: 60  # Request timeout in seconds
  
  # Timing and rate limiting
  sleep_time: 0.5  # Base delay between requests
  rate_limit:
    requests_per_minute: 120
    burst: 10  # Maximum burst requests
    
  # Error handling and retry logic
  max_retries: 5  # Maximum number of retry attempts
  retry_delay:
    min: 2  # Minimum retry delay in seconds
    max: 10  # Maximum retry delay in seconds
    
  # New job detection and resume settings
  lookback_pages: 5  # Number of pages to look back when resuming
  minimum_pages: 10  # Minimum pages to check before considering stopping
  max_empty_pages: 3  # Maximum consecutive empty pages before stopping
  max_resume_age: 86400  # Maximum age (in seconds) of previous state to resume from
  
  # State management
  state_tracking:
    enabled: true
    save_interval: 300  # Save state every 5 minutes
    backup_count: 3  # Number of state backups to keep
    
  # Timestamp handling
  timestamp_settings:
    format: "%Y-%m-%dT%H:%M:%S.%fZ"  # Expected timestamp format
    timezone: "UTC"
    
  # Job deduplication
  deduplication:
    enabled: true
    method: "id_based"  # Can be 'id_based' or 'content_based'
    cache_size: 10000  # Number of job IDs to keep in memory
    
  # Monitoring and logging
  monitoring:
    enabled: true
    metrics:
      - new_jobs_count
      - processing_time
      - memory_usage
      - request_latency
    alert_thresholds:
      max_errors_per_minute: 10
      max_memory_percent: 90
      max_latency_ms: 5000
      
  # Data validation
  validation:
    enabled: true
    required_fields:
      - id
      - title
      - posted_at
    
  # Output settings
  output:
    format: "json"
    compression: true
    batch_prefix: "batch_"
    timestamp_format: "%Y%m%d_%H%M%S"
    
  # Clean up settings
  cleanup:
    enabled: true
    max_age_days: 7  # Remove temporary files older than 7 days
    keep_last_n_batches: 50  # Always keep the last 50 batches
    
  # Debug settings
  debug:
    enabled: false
    verbose_logging: false
    save_raw_responses: false

